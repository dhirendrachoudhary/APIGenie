{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d50ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0138be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "JSON_FILE_PATH = f\"/Users/dhirendrachoudhary/Desktop/Workstation/Research/APIGenie/data/scikit-learn-api-reference.json\"\n",
    "CHROMA_DB_PATH = \"./chroma_db\" # Path to store ChromaDB files\n",
    "CHROMA_COLLECTION_NAME = \"sklearn_apis\"\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2' # or 'all-mpnet-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5c1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prepare_data import load_and_flatten_data\n",
    "api_document = load_and_flatten_data(JSON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e105b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save flattened data to a json\n",
    "with open(\"data/flattened_apis.json\", \"w\") as f:\n",
    "    json.dump(api_document, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhirendrachoudhary/miniconda3/envs/dev/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Embedding model loaded.\n",
      "Initializing ChromaDB client...\n",
      "Using existing collection: sklearn_apis\n",
      "Generating embeddings for 575 API documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 18/18 [00:03<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new documents to add. All documents might already exist.\n"
     ]
    }
   ],
   "source": [
    "from src.vectordb_embedding import (\n",
    "    initialize_embedding_model,\n",
    "    create_and_populate_vector_db,\n",
    "    retrieve_relevant_apis\n",
    ")\n",
    "\n",
    "# 2. Initialize embedding model\n",
    "embedding_model = initialize_embedding_model(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# 3. Create/Populate Vector DB\n",
    "api_collection = create_and_populate_vector_db(\n",
    "    api_document, embedding_model, CHROMA_DB_PATH, CHROMA_COLLECTION_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84aa2bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Build a classifier for multi-class text data, data is sparse\n",
      "\n",
      "User Query: 'Build a classifier for multi-class text data, data is sparse'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test_queries = [\n",
    "    \"Build a classifier for multi-class text data, data is sparse\",\n",
    "    \"I need to preprocess numerical features that have different scales, preparing for an SVM.\",\n",
    "    \"Find a clustering algorithm suitable for a large number of samples and features.\",\n",
    "    \"How to perform feature selection to improve my regression model?\",\n",
    "    \"Combine preprocessing and a classification model into a single unit.\"\n",
    "]\n",
    "\n",
    "# 4. Retrieve relevant APIs\n",
    "print(f\"Query: {test_queries[0]}\")\n",
    "# def retrieve_relevant_apis(query_text, model, collection, n_results=5):\n",
    "relevant_apis = retrieve_relevant_apis(\n",
    "    test_queries[0], embedding_model, api_collection, n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ae40423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['121', '177', '108', '175', '423']],\n",
       " 'embeddings': None,\n",
       " 'documents': [[\"API Name: LatentDirichletAllocation. Belongs to module: sklearn.decomposition. Signature: class sklearn.decomposition.LatentDirichletAllocation(n_components=10, *, doc_topic_prior=None, topic_word_prior=None, learning_method='batch', learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=None, verbose=0, random_state=None). Example Usage: from sklearn.decomposition import LatentDirichletAllocation\\nfrom sklearn.datasets import make_multilabel_classification\\n# This produces a feature matrix of token counts, similar to what\\n# CountVectorizer would produce on text.\\nX, _ = make_multilabel_classification(random_state=0)\\nlda = LatentDirichletAllocation(n_components=5,\\n    random_state=0)\\nlda.fit(X)\\nLatentDirichletAllocation(...)\\n# get topics for some given samples:\\nlda.transform(X[-2:])\\narray([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\\n       [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])\",\n",
       "   \"API Name: TfidfTransformer. Belongs to module: sklearn.feature_extraction. Signature: class sklearn.feature_extraction.text.TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False). Example Usage: from sklearn.feature_extraction.text import TfidfTransformer\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.pipeline import Pipeline\\ncorpus = ['this is the first document',\\n          'this document is the second document',\\n          'and this is the third one',\\n          'is this the first document']\\nvocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\\n              'and', 'one']\\npipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\\n                 ('tfid', TfidfTransformer())]).fit(corpus)\\npipe['count'].transform(corpus).toarray()\\narray([[1, 1, 1, 1, 0, 1, 0, 0],\\n       [1, 2, 0, 1, 1, 1, 0, 0],\\n       [1, 0, 0, 1, 0, 1, 1, 1],\\n       [1, 1, 1, 1, 0, 1, 0, 0]])\\npipe['tfid'].idf_\\narray([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\\n       1.        , 1.91629073, 1.91629073])\\npipe.transform(corpus).shape\\n(4, 8)\",\n",
       "   \"API Name: make_multilabel_classification. Belongs to module: sklearn.datasets. Signature: class sklearn.datasets.make_multilabel_classification(n_samples=100, n_features=20, *, n_classes=5, n_labels=2, length=50, allow_unlabeled=True, sparse=False, return_indicator='dense', return_distributions=False, random_state=None). Example Usage: from sklearn.datasets import make_multilabel_classification\\nX, y = make_multilabel_classification(n_labels=3, random_state=42)\\nX.shape\\n(100, 20)\\ny.shape\\n(100, 5)\\nlist(y[:3])\\n[array([1, 1, 0, 1, 0]), array([0, 1, 1, 1, 0]), array([0, 1, 0, 0, 0])]\",\n",
       "   \"API Name: CountVectorizer. Belongs to module: sklearn.feature_extraction. Signature: class sklearn.feature_extraction.text.CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\\\\\b\\\\\\\\w\\\\\\\\w+\\\\\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>). Example Usage: from sklearn.feature_extraction.text import CountVectorizer\\ncorpus = [\\n    'This is the first document.',\\n    'This document is the second document.',\\n    'And this is the third one.',\\n    'Is this the first document?',\\n]\\nvectorizer = CountVectorizer()\\nX = vectorizer.fit_transform(corpus)\\nvectorizer.get_feature_names_out()\\narray(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\\n       'this'], ...)\\nprint(X.toarray())\\n[[0 1 1 1 0 0 1 0 1]\\n [0 2 0 1 0 1 1 0 1]\\n [1 0 0 1 1 0 1 1 1]\\n [0 1 1 1 0 0 1 0 1]]\\nvectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\\nX2 = vectorizer2.fit_transform(corpus)\\nvectorizer2.get_feature_names_out()\\narray(['and this', 'document is', 'first document', 'is the', 'is this',\\n       'second document', 'the first', 'the second', 'the third', 'third one',\\n       'this document', 'this is', 'this the'], ...)\\n >>> print(X2.toarray())\\n [[0 0 1 1 0 0 1 0 0 0 0 1 0]\\n [0 1 0 1 0 1 0 1 0 0 1 0 0]\\n [1 0 0 1 0 0 0 0 1 1 0 1 0]\\n [0 0 1 0 1 0 1 0 0 0 0 0 1]]\",\n",
       "   'API Name: MultiOutputClassifier. Belongs to module: sklearn.multioutput. Signature: class sklearn.multioutput.MultiOutputClassifier(estimator, *, n_jobs=None). Example Usage: import numpy as np\\nfrom sklearn.datasets import make_multilabel_classification\\nfrom sklearn.multioutput import MultiOutputClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nX, y = make_multilabel_classification(n_classes=3, random_state=0)\\nclf = MultiOutputClassifier(LogisticRegression()).fit(X, y)\\nclf.predict(X[-2:])\\narray([[1, 1, 1],\\n       [1, 0, 1]])']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'module_name': 'sklearn.decomposition',\n",
       "    'api_full_name': 'sklearn.decomposition.LatentDirichletAllocation',\n",
       "    'class_name': 'LatentDirichletAllocation',\n",
       "    'link': 'https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html',\n",
       "    'signature': \"class sklearn.decomposition.LatentDirichletAllocation(n_components=10, *, doc_topic_prior=None, topic_word_prior=None, learning_method='batch', learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=None, verbose=0, random_state=None)\"},\n",
       "   {'api_full_name': 'sklearn.feature_extraction.TfidfTransformer',\n",
       "    'class_name': 'TfidfTransformer',\n",
       "    'signature': \"class sklearn.feature_extraction.text.TfidfTransformer(*, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\",\n",
       "    'link': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html',\n",
       "    'module_name': 'sklearn.feature_extraction'},\n",
       "   {'signature': \"class sklearn.datasets.make_multilabel_classification(n_samples=100, n_features=20, *, n_classes=5, n_labels=2, length=50, allow_unlabeled=True, sparse=False, return_indicator='dense', return_distributions=False, random_state=None)\",\n",
       "    'api_full_name': 'sklearn.datasets.make_multilabel_classification',\n",
       "    'link': 'https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_multilabel_classification.html',\n",
       "    'class_name': 'make_multilabel_classification',\n",
       "    'module_name': 'sklearn.datasets'},\n",
       "   {'link': 'https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html',\n",
       "    'module_name': 'sklearn.feature_extraction',\n",
       "    'class_name': 'CountVectorizer',\n",
       "    'signature': \"class sklearn.feature_extraction.text.CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\\\\\b\\\\\\\\w\\\\\\\\w+\\\\\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\",\n",
       "    'api_full_name': 'sklearn.feature_extraction.CountVectorizer'},\n",
       "   {'api_full_name': 'sklearn.multioutput.MultiOutputClassifier',\n",
       "    'module_name': 'sklearn.multioutput',\n",
       "    'link': 'https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html',\n",
       "    'signature': 'class sklearn.multioutput.MultiOutputClassifier(estimator, *, n_jobs=None)',\n",
       "    'class_name': 'MultiOutputClassifier'}]],\n",
       " 'distances': [[1.1976370811462402,\n",
       "   1.2764344215393066,\n",
       "   1.3092079162597656,\n",
       "   1.3336966037750244,\n",
       "   1.337829828262329]]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2fddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
